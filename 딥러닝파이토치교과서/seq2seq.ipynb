{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B_vQYlBp_5U6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비"
      ],
      "metadata": {
        "id": "kRkLRnI-BHzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0 # 문장의 시작\n",
        "EOS_token = 1 # 문장의 끝\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0:'SOS', 1:'EOS'}\n",
        "    self.n_words = 2 # SOS, EOS\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2index[word] += 1"
      ],
      "metadata": {
        "id": "Dslkp5Q7BGYZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 정규화"
      ],
      "metadata": {
        "id": "crq31Yn2CJQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_table('/content/drive/MyDrive/pytorch/kor.txt', header=None, names=['kor','eng','ex'])\n",
        "data = data[['kor', 'eng']]\n",
        "data.to_csv('/content/drive/MyDrive/pytorch/kor.csv')"
      ],
      "metadata": {
        "id": "-bpTXFF8EgnN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeString(df, lang):\n",
        "  sentence = df[lang].str.lower()\n",
        "  sentence = sentence.str.replace('[^A-Za-z\\s]+', ' ')\n",
        "  sentence = sentence.str.normalize('NFD')\n",
        "  sentenct = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "  return sentence\n",
        "\n",
        "def read_sentence(df, lang1, lang2):\n",
        "  sentence1 = normalizeString(df, lang1)\n",
        "  sentence2 = normalizeString(df, lang2)\n",
        "  return sentence1, sentence2\n",
        "\n",
        "def read_file(loc, lang1, lang2):\n",
        "  df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n",
        "  return df\n",
        "\n",
        "def process_data(lang1, lang2):\n",
        "  df = read_file('./content/drive/MyDrive/pytorch/kor.txt')\n",
        "  sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
        "\n",
        "  input_lang = Lang()\n",
        "  output_lang = Lang()\n",
        "  pairs = []\n",
        "  for i in range(len(df)):\n",
        "    if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
        "      full = [sentence1[i], sentence2[i]]\n",
        "      input_lang.addSentence(sentence1[i])\n",
        "      output_lang.addSentence(sentence2[i])\n",
        "      pairs.append(full)\n",
        "\n",
        "  return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "t4z56xwdCIAC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텐서로 변환"
      ],
      "metadata": {
        "id": "3VOS39h1GMqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "  indexes = indexesFromSentence(lang, sentence)\n",
        "  indexes.append(EOS_token)\n",
        "  return torch.tensor(indexes, dtype=torch.long, device=device).view(-1,1)\n",
        "\n",
        "def tensorFromPair(input_lang, output_lang, pair):\n",
        "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "  output_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "  return (input_tensor, output_tensor)"
      ],
      "metadata": {
        "id": "iq33ahzWDMQ-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인코더 네트워크"
      ],
      "metadata": {
        "id": "YBlKsCGHHLyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers,):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embbed_dim = embbed_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.embeding = nn.Embedding(input_dim, self.embbed_dim)\n",
        "    self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "\n",
        "  def forward(self, src):\n",
        "    embedded = self.embedding(src).view(1,1,-1)\n",
        "    outputs, hidden = self.gru(embedded)\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "jq_e4qrhHLBo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 디코더 네트워크"
      ],
      "metadata": {
        "id": "03GeToa7KJ-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embbed_dim = embbed_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
        "    self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "    self.out = nn.Linear(self.hidden_dim, output_dim)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    input = input.view(1,-1)\n",
        "    embedded = F.relu(self.embedding(input))\n",
        "    output, hidden = self.gru(embedded, hidden)\n",
        "    prediction = self.softmax(self.out(output[0]))\n",
        "    return prediction, hidden"
      ],
      "metadata": {
        "id": "pXYsULEqIUkp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CMFknAo3LAJW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}